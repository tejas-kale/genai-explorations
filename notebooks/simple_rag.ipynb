{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94922cef",
   "metadata": {},
   "source": [
    "# Simple Retrieval Augmented Generation (RAG) System\n",
    "\n",
    "In this notebook, we will build a simple RAG system to answer questions based on Markdown files. As an [Obsidian](https://obsidian.md) user, all my notes are stored as Markdown files in a directory that is called a *vault* in Obsidian's parlance. The idea is to build a system that: \n",
    "1. Reads these files\n",
    "2. Splits them into chunks if required\n",
    "3. Converts them into embeddings\n",
    "4. Stores the embeddings in a persistent database called ChromaDB\n",
    "5. Retrieves chunks from the database relevant to the query\n",
    "6. Asks the query to a LLM with the retrieved chunk provided as context\n",
    "\n",
    "We begin by loading the required packages and creating an OpenAI client which we will then use to ask the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7aabaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import chromadb\n",
    "from chromadb import Collection\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dfd2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"../secrets.env\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b1147",
   "metadata": {},
   "source": [
    "Next, we define the root directory of a Obsidian vault and then iteratively load the Markdown files in the vault. In order to enable filtering of files based on their location, we add the file's relative path as another key in the output dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ab04c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAULT_DIR = (\n",
    "    \"/Users/tejaskale/Library/Mobile Documents/iCloud~md~obsidian/Documents/RAG Test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c7109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_markdown_files(vault_dir: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Recursively loads all Markdown (.md) files from the specified Obsidian vault\n",
    "    directory.\n",
    "\n",
    "    Args:\n",
    "        vault_dir (str): Path to the root directory of the Obsidian vault.\n",
    "\n",
    "    Returns:\n",
    "        list of dict: A list where each element is a dictionary with 'content'\n",
    "        (str) and 'path' (str) keys, representing the file content and its relative path.\n",
    "    \"\"\"\n",
    "    md_files = []\n",
    "    for path in pathlib.Path(vault_dir).rglob(\"*.md\"):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        # Exclude the file name from the relative path (keep only the directory)\n",
    "        relative_path = str(path.relative_to(vault_dir).parent.as_posix())\n",
    "        md_files.append({\"content\": content, \"path\": relative_path})\n",
    "    return md_files\n",
    "\n",
    "\n",
    "markdown_files = load_markdown_files(VAULT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39db789",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d8bf97",
   "metadata": {},
   "source": [
    "Having loaded the required files, we know split each file into chunks, tokenise each chunk using OpenAI's embeddings, and then save the embeddings to a persistent ChromaDB vector store. In this example, we begin with the default value to *chunk size* and *chunk overlap* that was provided in the tutorials. If the answers were get from the LLM do not match our expectation, these parameters can be tweaked for improvements.\n",
    "\n",
    "While creating documents in ChromaDB, we specify the file path as metadata for each document. This gives us the ability to filter data based on the file path directory and thus save the overhead costs of searching through everything and potentially getting unrelated answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b7020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(\n",
    "    text: str, chunk_size: Optional[int] = 1000, chunk_overlap: Optional[int] = 200\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits text into chunks using Langchain's RecursiveCharacterTextSplitter.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to split.\n",
    "        chunk_size (int, optional): The maximum size of each chunk. Defaults to 1000.\n",
    "        chunk_overlap (int, optional): The number of overlapping characters between\n",
    "        chunks. Defaults to 200.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "\n",
    "def embed_and_store_markdown_files(\n",
    "    markdown_files: List[Dict[str, str]],\n",
    "    client: OpenAI,\n",
    "    persist_directory: str = \"./chroma_db\",\n",
    "    collection_name: str = \"obsidian_vault\",\n",
    ") -> Collection:\n",
    "    \"\"\"\n",
    "    Splits each markdown file into chunks, generates embeddings for each chunk using the provided OpenAI client,\n",
    "    and stores the embeddings along with the chunk text and metadata in a persistent ChromaDB collection.\n",
    "\n",
    "    Args:\n",
    "        markdown_files (list): List of dictionaries, each containing 'content' (str)\n",
    "            and 'path' (str) for a markdown file.\n",
    "        client (OpenAI): An OpenAI client instance for generating embeddings.\n",
    "        persist_directory (str, optional): Directory path to persist the ChromaDB\n",
    "            database. Defaults to \"./chroma_db\".\n",
    "        collection_name (str, optional): Name of the ChromaDB collection. Defaults\n",
    "            to \"obsidian_vault\".\n",
    "\n",
    "    Returns:\n",
    "        Collection: The ChromaDB collection cursor after storing all embeddings.\n",
    "    \"\"\"\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_directory)\n",
    "    collection = chroma_client.get_or_create_collection(collection_name)\n",
    "    for idx, file in enumerate(markdown_files):\n",
    "        chunks = split_text(file[\"content\"])\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            response = client.embeddings.create(\n",
    "                input=chunk, model=\"text-embedding-ada-002\"\n",
    "            )\n",
    "            embedding = response.data[0].embedding\n",
    "            collection.add(\n",
    "                embeddings=[embedding],\n",
    "                documents=[chunk],\n",
    "                metadatas=[{\"path\": file[\"path\"]}],\n",
    "                ids=[f\"{idx}_{i}\"],\n",
    "            )\n",
    "\n",
    "    return collection\n",
    "\n",
    "\n",
    "# Run the embedding and storage process, and get the collection cursor\n",
    "collection = embed_and_store_markdown_files(markdown_files, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a92777d",
   "metadata": {},
   "source": [
    "Having stores vector embeddings our our documents in a database, we now write a function that takes a user query and optional metadata filter from the user and uses it to filter for n documents from the ChromaDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba74582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_documents(\n",
    "    query: str,\n",
    "    client: OpenAI,\n",
    "    collection: Collection,\n",
    "    n_results: int = 5,\n",
    "    metadata_filter: Optional[Dict[str, str]] = None,\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Retrieves the top n most relevant documents from the ChromaDB collection for a given query,\n",
    "    optionally filtering by metadata.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "        client (OpenAI): An OpenAI client instance for generating the query embedding.\n",
    "        collection (Collection): The ChromaDB collection to search.\n",
    "        n_results (int, optional): Number of top documents to retrieve. Defaults to 5.\n",
    "        metadata_filter (dict, optional): Metadata filter for narrowing down results.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: List of dictionaries with 'document' and 'metadata' keys.\n",
    "    \"\"\"\n",
    "    # Generate embedding for the query\n",
    "    response = client.embeddings.create(input=query, model=\"text-embedding-ada-002\")\n",
    "    query_embedding = response.data[0].embedding\n",
    "\n",
    "    # Query the ChromaDB collection\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        where=metadata_filter if metadata_filter else None,\n",
    "        include=[\"documents\", \"metadatas\"],\n",
    "    )\n",
    "\n",
    "    # Format the results\n",
    "    docs = []\n",
    "    for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "        docs.append({\"document\": doc, \"metadata\": meta})\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f1ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_relevant_documents(\n",
    "    \"What is the most important mindset for vibe coding?\", client, collection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd6c504",
   "metadata": {},
   "source": [
    "With the relevant context now available, we now define a function that takes the query as input, retrieves the relevant documents, creates a prompt with the query and the documents, and asks the LLM to provide an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4291b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query_with_context(\n",
    "    query: str,\n",
    "    client: OpenAI,\n",
    "    collection: Collection,\n",
    "    n_results: int = 5,\n",
    "    metadata_filter: Optional[Dict[str, str]] = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Answers a user query by retrieving relevant documents from the ChromaDB collection,\n",
    "    constructing a prompt with the query and context, and querying GPT-4o mini for a response.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "        client (OpenAI): An OpenAI client instance.\n",
    "        collection (Collection): The ChromaDB collection to search.\n",
    "        n_results (int, optional): Number of top documents to retrieve. Defaults to 5.\n",
    "        metadata_filter (Optional[Dict[str, str]], optional): Metadata filter for narrowing down results.\n",
    "\n",
    "    Returns:\n",
    "        str: The response generated by GPT-4o mini.\n",
    "    \"\"\"\n",
    "    relevant_docs = retrieve_relevant_documents(\n",
    "        query, client, collection, n_results=n_results, metadata_filter=metadata_filter\n",
    "    )\n",
    "\n",
    "    context = \"\\n\\n\".join(doc[\"document\"] for doc in relevant_docs)\n",
    "    prompt = (\n",
    "        f\"Answer the following question using the provided context.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    You are a helpful assistant who answers questions based only on the provided context.\n",
    "    If the answer is not directly available in the context, respond with:\n",
    "    'The answer is not available in the provided context.'\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        max_tokens=512,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d0bab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_query_with_context(\n",
    "    \"What is the most important mindset for vibe coding?\", client, collection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c0b4bd",
   "metadata": {},
   "source": [
    "## Resources\n",
    "For this notebook, I relied on two sources:\n",
    "- [Module 1](https://github.com/DataTalksClub/llm-zoomcamp/tree/main/01-intro) of the course LLM Zoomcamp\n",
    "- [Simple RAG](https://github.com/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_rag.ipynb) notebook by Nir Diamant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-explorations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
