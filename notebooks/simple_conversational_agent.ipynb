{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2695b81",
   "metadata": {},
   "source": [
    "# Simple Conversational Agent\n",
    "In this notebook, we will build a simple agent that is able to refer to the history of the interaction is order to provide relevant and succinct answers. It consists of:\n",
    "* LLM client (LM Studio) to get answers from a large language model\n",
    "* Customised prompt that consists of the user question and the conversation history\n",
    "* History manager to manage the conversation history and context\n",
    "* Message store to hold the messages for each conversation session.\n",
    "\n",
    "As usual, we begin by loading the required packages and creating the LLM client. To set up LM Studio to serve a local model via an API, check out their guide [Run LM Studio as a service (headless)](https://lmstudio.ai/docs/app/api/headless)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d10a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23beea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_studio_base_url = \"http://localhost:1234/v1\"\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_base=lm_studio_base_url,\n",
    "    max_tokens=1000,\n",
    "    api_key=\"not-needed\",\n",
    "    model_name=\"gemma-3-4b-it\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672d5c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ca7912",
   "metadata": {},
   "source": [
    "Having verified that the LLM via LM Studio is responding correctly, we now create a simple chat history store which is kept in-memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed1ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "\n",
    "def get_chat_history(session_id: str) -> ChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4fff88",
   "metadata": {},
   "source": [
    "Next, we create a prompt template that includes a system prompt, the conversation history, and the user's current question. To provide the conversation history, we include a placeholder which assumes that the history is in a dictionary with the specified key which is `history` in this context. To construct a prompt using this template, unless `optional` is set to `True`, it is assumed that the dictionary with the key is *already present*. But since the first message in the conversation has no history, it is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f309ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\", optional=True),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb6a01a",
   "metadata": {},
   "source": [
    "We now combine the history manager and prompt in a runnable chain i.e. we define a pipeline for how the functions should be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163da240",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1a6cc4",
   "metadata": {},
   "source": [
    "Finally, we wrap the chain, also called a *runnable* in Langchain parlance, in another runnable called `RunnableWithMessageHistory`. This class takes the runnable and the session history retrieval function as input. It then takes the responsibility of retrieving the history, given the session ID, of a given chat and updating it after every message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eebd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    runnable=chain,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_message_key=\"input\",\n",
    "    history_message_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683da62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = \"test_123\"\n",
    "\n",
    "response_1 = chain_with_history.invoke(\n",
    "    {\"input\": \"When was Karim Benzema born?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": session_id},\n",
    "    },\n",
    ")\n",
    "print(f\"AI: {response_1.content}\")\n",
    "\n",
    "response_2 = chain_with_history.invoke(\n",
    "    {\"input\": \"Which club did he play the longest for?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": session_id},\n",
    "    },\n",
    ")\n",
    "print(f\"AI: {response_2.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a38292",
   "metadata": {},
   "outputs": [],
   "source": [
    "store[\"test_123\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-explorations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
