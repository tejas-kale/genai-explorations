{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55808996",
   "metadata": {},
   "source": [
    "# Creating Audiobooks with Gemini 2.5 TTS\n",
    "Previously, I [experimented](exploring_gemini_25_tts.ipynb) with the Gemini-2.5 Flash Preview TTS model by generating speech from simple text in a markdown file. Encouraged by the experience, I wanted to create another notebook where I read an ePub file instead of a markdown file and convert the entire ePub into speech. In this notebook, we will go through the process of creating \"audiobooks\" with the Gemini 2.5 Flash Preview TTS model.\n",
    "\n",
    "Every quarter, I receive a couple of magazines in ePub format that I do not often get a chance to read. These magazines are available in the ePub format which is a popular open standard for encoding data and metadata about complex texts like magazines and books. In this notebook, we will explore how to load these ePub files in Python, use the Gemini 2.0 Flash model to extract metadata from certain contents of the file, generate speech for the text, and finally save all generated audio along with chapter markers to a file. This audiobook can then be read on a supported app like *Book Player* on the iPhone.\n",
    "\n",
    "First, let us load the required packages, set our secrets, and create a `genai` client. We can use the `uv` package for installing the packages and creating the necessary Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133bda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import ebooklib\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from ebooklib import epub\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa6dfd6",
   "metadata": {},
   "source": [
    "## Create Gemini client\n",
    "We now load our API key and then create the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e7f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"../secrets.env\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be7a4dc",
   "metadata": {},
   "source": [
    "## Define prompts\n",
    "In this notebook, we use prompts for 2 scenarios:\n",
    "1. Use a Gemini 2.0 Flash model to tell if a given text contains a genuine magazine article or other content like advertisements, crosswords, etc. If the text constains an article, extract its title, sub-headline, author and the starting point of the body of the text.\n",
    "2. Instruct Gemini 2.5 Flash Preview TTS on how to read the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d3dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_CHAPTER_CLASSIFY_AND_EXTRACT_PROMPT = \"\"\"\n",
    "You are an assistant for audiobook creation. Given the first 20 lines of a \n",
    "chapter from an ePub, do the following:\n",
    "1. Determine if the content is an article (story, essay, or feature) or \n",
    "something else (advertisement, crossword, non-story, etc).\n",
    "2. If it is an article, extract and return:\n",
    "    - The headline (main title)\n",
    "    - The sub-headline (subtitle, if present, else empty string)\n",
    "    - The author (if present, else empty string)\n",
    "    - The line number (0-based) from which the actual article body starts \n",
    "    (i.e., after headline, sub-headline, and author)\n",
    "3. If it is not an article, do not extract any details.\n",
    "Respond in the following JSON format:\n",
    "{\n",
    "    \"is_article\": true/false,\n",
    "    \"headline\": \"...\",\n",
    "    \"sub_headline\": \"...\",\n",
    "    \"author\": \"...\",\n",
    "    \"body_start_line\": int\n",
    "}\n",
    "If it is not an article, set is_article to false and leave other fields \n",
    "empty or null.\n",
    "Here are the first 20 lines of the chapter (one per line):\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_FOR_READING = \"Read in an even tone with a West London accent.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a2e15",
   "metadata": {},
   "source": [
    "## Read ePub\n",
    "An ePub file is a special XML-based open-format that allows us to pack data and metadata into a single file. It is popularly used for books, magazine, and other kinds of reports. In an ePub file, different chapters or sections are denoted in two ways:\n",
    "1. Each chapter contains its own HTML file in an archive that is the ePub file.\n",
    "2. The file contains a table of contents which links to individual chapters or sections.\n",
    "\n",
    "Using the `ebooklib` package, we define a function that can the ePub file, iterate over its chapters and then use the Gemini API to get further metadata about each chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7856726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_body(text_lines: list, body_start: int) -> str:\n",
    "    \"\"\"\n",
    "    Extract the body text from the chapter, starting from the specified start line.\n",
    "    Args:\n",
    "        text_lines (list): List of non-empty lines from the chapter.\n",
    "        body_start (int): The line number to start the body text extraction.\n",
    "    Returns:\n",
    "        str: The extracted body text.\n",
    "    \"\"\"\n",
    "    return \"\\n\".join(text_lines[body_start:]).strip()\n",
    "\n",
    "\n",
    "def gemini_classify_and_extract_chapter(client, lines):\n",
    "    \"\"\"\n",
    "    Use Gemini to classify and extract article metadata from the first lines\n",
    "    of a chapter.\n",
    "\n",
    "    Args:\n",
    "        client: Gemini client.\n",
    "        lines (list[str]): First 20 lines of the chapter.\n",
    "    Returns:\n",
    "        dict: {is_article, headline, sub_headline, author, body_start_line}\n",
    "    \"\"\"\n",
    "    prompt = GEMINI_CHAPTER_CLASSIFY_AND_EXTRACT_PROMPT + \"\\n\" + \"\\n\".join(lines[:20])\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(temperature=0.0, max_output_tokens=256),\n",
    "    )\n",
    "\n",
    "    # Remove code block markers if present\n",
    "    text = response.candidates[0].content.parts[0].text.strip()\n",
    "    if text.startswith(\"```\"):\n",
    "        text = text.split(\"\\n\", 1)[-1]\n",
    "    if text.endswith(\"```\"):\n",
    "        text = text.rsplit(\"\\n\", 1)[0]\n",
    "\n",
    "    try:\n",
    "        result = json.loads(text)\n",
    "    except Exception:\n",
    "        raise ValueError(f\"Could not parse Gemini response: {text}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def read_epub_by_chapters(epub_path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Reads an ePub file and returns a dictionary of chapter headlines to cleaned text,\n",
    "    extracting headline, sub-headline, author, and body using Gemini.\n",
    "    Args:\n",
    "        epub_path (str): Path to the ePub file.\n",
    "    Returns:\n",
    "        dict: {headline: chapter_text}\n",
    "    \"\"\"\n",
    "    book = epub.read_epub(epub_path)\n",
    "    candidate_chapters = []\n",
    "    for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
    "        soup = BeautifulSoup(item.get_content(), \"html.parser\")\n",
    "        for tag in soup([\"img\", \"script\", \"style\"]):\n",
    "            tag.decompose()\n",
    "\n",
    "        text_lines = [l.strip() for l in soup.get_text(\"\\n\").split(\"\\n\") if l.strip()]\n",
    "        if not text_lines:\n",
    "            continue\n",
    "\n",
    "        # Use Gemini to classify and extract metadata\n",
    "        meta = gemini_classify_and_extract_chapter(client, text_lines)\n",
    "        if not meta.get(\"is_article\"):\n",
    "            continue\n",
    "        headline = meta.get(\"headline\") or \"\"\n",
    "        sub_headline = meta.get(\"sub_headline\") or \"\"\n",
    "        author = meta.get(\"author\") or \"\"\n",
    "        body_start = meta.get(\"body_start_line\") or 0\n",
    "        body = \"\\n\".join(text_lines[body_start:]).strip()\n",
    "\n",
    "        if headline and body:\n",
    "            chapter_text = headline + \"\\n\\n\"\n",
    "            if sub_headline:\n",
    "                chapter_text += sub_headline + \"\\n\\n\"\n",
    "            if author:\n",
    "                chapter_text += author + \"\\n\\n\"\n",
    "            chapter_text += body\n",
    "            candidate_chapters.append((headline, chapter_text))\n",
    "\n",
    "    filtered_chapters = {}\n",
    "    for headline, chapter_text in candidate_chapters:\n",
    "        filtered_chapters[headline] = chapter_text\n",
    "\n",
    "    return filtered_chapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e166a17",
   "metadata": {},
   "source": [
    "## Split text\n",
    "Since the Gemini model only accepts up to 8000 input tokens and each token is approximately 4 characters long, we define a function that splits text into chunks of up to 7000 tokens, adding two empty lines at the end of each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e366c18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_to_chunks(\n",
    "    text: str, max_tokens: int = 7000, token_chars: int = 4\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Split the input text into chunks, each with a maximum number of tokens.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to split.\n",
    "        max_tokens (int): Maximum number of tokens per chunk (default: 7000).\n",
    "        token_chars (int): Number of characters per token (default: 4).\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of text chunks, each ending with two newlines.\n",
    "    \"\"\"\n",
    "    max_chars = max_tokens * token_chars\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + max_chars, len(text))\n",
    "        chunk = text[start:end].rstrip() + \"\\n\\n\"\n",
    "        chunks.append(chunk)\n",
    "        start = end\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2af1e8d",
   "metadata": {},
   "source": [
    "## Create audiobook\n",
    "Finally, we define the function creates the audiobooks. First, grouped by chapter, it joins all the sound bytes for a given chapter. It then creates a file with the `_chapter.txt` suffix with information about timestamps of each article. In order to play the audiobook with chapters on mobile device, we need to use `ffmpeg` and hence a terminal command is printed which should make it easy to generated the desired audiobook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e89b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_pcm_by_chapter(\n",
    "    pcm_chunks: List[bytes],\n",
    "    chapters: List[str],\n",
    ") -> Tuple[List[str], Dict[str, List[bytes]]]:\n",
    "    \"\"\"\n",
    "    Groups consecutive PCM audio chunks by chapter name.\n",
    "\n",
    "    Args:\n",
    "        pcm_chunks (List[bytes]): Flat list of PCM audio data for all chapter chunks.\n",
    "        chapters (List[str]): Flat list of chapter names, same length as pcm_chunks.\n",
    "        Consecutive repeats for the same chapter.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], Dict[str, List[bytes]]]:\n",
    "            - List of chapter names in order of appearance (no duplicates).\n",
    "            - Dictionary mapping chapter name to list of PCM chunks\n",
    "            (consecutive for that chapter).\n",
    "    \"\"\"\n",
    "    chapter_audio_dict: Dict[str, List[bytes]] = {}\n",
    "    chapter_order: List[str] = []\n",
    "    prev_chapter = None\n",
    "    for ch, pcm in zip(chapters, pcm_chunks):\n",
    "        if ch != prev_chapter:\n",
    "            chapter_order.append(ch)\n",
    "            chapter_audio_dict[ch] = []\n",
    "        chapter_audio_dict[ch].append(pcm)\n",
    "        prev_chapter = ch\n",
    "    return chapter_order, chapter_audio_dict\n",
    "\n",
    "\n",
    "def join_chapter_audio(\n",
    "    chapter_order: List[str],\n",
    "    chapter_audio_dict: Dict[str, List[bytes]],\n",
    "    sample_width: int,\n",
    "    rate: int,\n",
    "    channels: int,\n",
    ") -> List[Tuple[str, \"AudioSegment\"]]:\n",
    "    \"\"\"\n",
    "    Joins PCM chunks for each chapter into a single AudioSegment.\n",
    "\n",
    "    Args:\n",
    "        chapter_order (List[str]): List of chapter names in order.\n",
    "        chapter_audio_dict (Dict[str, List[bytes]]): Dict mapping chapter name to list of PCM chunks.\n",
    "        sample_width (int): Sample width in bytes.\n",
    "        rate (int): Sample rate in Hz.\n",
    "        channels (int): Number of audio channels.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, AudioSegment]]: List of (chapter name, joined AudioSegment) tuples in order.\n",
    "    \"\"\"\n",
    "    chapter_audio_segments: List[Tuple[str, \"AudioSegment\"]] = []\n",
    "    for ch in chapter_order:\n",
    "        segs = [\n",
    "            AudioSegment(\n",
    "                data=pcm, sample_width=sample_width, frame_rate=rate, channels=channels\n",
    "            )\n",
    "            for pcm in chapter_audio_dict[ch]\n",
    "        ]\n",
    "        chapter_audio = sum(segs)\n",
    "        chapter_audio_segments.append((ch, chapter_audio))\n",
    "    return chapter_audio_segments\n",
    "\n",
    "\n",
    "def write_chapters_txt(\n",
    "    filename: str, chapter_audio_segments: List[Tuple[str, \"AudioSegment\"]]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Writes a chapters.txt file for ffmpeg using chapter start/end times and titles.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Output audiobook filename (used to derive chapters.txt name).\n",
    "        chapter_audio_segments (List[Tuple[str, AudioSegment]]): List of\n",
    "        (chapter name, AudioSegment) tuples.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the generated chapters.txt file.\n",
    "    \"\"\"\n",
    "    chapter_times: List[int] = []\n",
    "    current_time = 0\n",
    "    for _, seg in chapter_audio_segments:\n",
    "        chapter_times.append(current_time)\n",
    "        current_time += len(seg)\n",
    "    full_audio_len = current_time\n",
    "    chapters_txt = filename.rsplit(\".\", 1)[0] + \"_chapters.txt\"\n",
    "    with open(chapters_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, (start_ms, (ch, _)) in enumerate(\n",
    "            zip(chapter_times, chapter_audio_segments)\n",
    "        ):\n",
    "            f.write(f\"[CHAPTER]\\nTIMEBASE=1/1000\\nSTART={start_ms}\\n\")\n",
    "            end_ms = (\n",
    "                chapter_times[i + 1] if i + 1 < len(chapter_times) else full_audio_len\n",
    "            )\n",
    "            f.write(f\"END={end_ms}\\nTITLE={ch}\\n\\n\")\n",
    "    print(f\"Chapters file saved as {chapters_txt}.\")\n",
    "    return chapters_txt\n",
    "\n",
    "\n",
    "def save_audiobook(\n",
    "    filename: str,\n",
    "    pcm_chunks: List[bytes],\n",
    "    chapters: List[str],\n",
    "    channels: int = 1,\n",
    "    rate: int = 24000,\n",
    "    sample_width: int = 2,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save concatenated PCM audio data to an M4B (AAC) audiobook file and generate\n",
    "    a chapters.txt file for ffmpeg. Consecutive identical chapter names are grouped,\n",
    "    their PCM chunks joined, and one entry per chapter is created in chapters.txt.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Output M4B file name.\n",
    "        pcm_chunks (List[bytes]): Flat list of PCM audio data for all chapter chunks.\n",
    "        chapters (List[str]): Flat list of chapter names, same length as pcm_chunks.\n",
    "        channels (int, optional): Number of audio channels. Defaults to 1.\n",
    "        rate (int, optional): Sample rate in Hz. Defaults to 24000.\n",
    "        sample_width (int, optional): Sample width in bytes. Defaults to 2.\n",
    "    \"\"\"\n",
    "    if not filename.lower().endswith(\".m4b\"):\n",
    "        filename = filename.rsplit(\".\", 1)[0] + \".m4b\"\n",
    "    chapter_order, chapter_audio_dict = group_pcm_by_chapter(pcm_chunks, chapters)\n",
    "    chapter_audio_segments = join_chapter_audio(\n",
    "        chapter_order, chapter_audio_dict, sample_width, rate, channels\n",
    "    )\n",
    "    # Export as M4B (AAC in M4B container)\n",
    "    full_audio = sum(seg for _, seg in chapter_audio_segments)\n",
    "    full_audio.export(filename, format=\"mp4\", codec=\"aac\")\n",
    "    print(f\"Audiobook saved as {filename}.\")\n",
    "    chapters_txt = write_chapters_txt(filename, chapter_audio_segments)\n",
    "    print(\"To mux chapters into the M4B, use:\")\n",
    "    print(\n",
    "        f\"ffmpeg -i '{filename}' -f ffmetadata -i '{chapters_txt}' -map_metadata 1 -codec copy '{filename.rsplit('.', 1)[0]}_with_chapters.m4b'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b4128d",
   "metadata": {},
   "source": [
    "## Prepare chunks\n",
    "Having defined all the relevant functions, we now load the ePub file and convert its contents into chunks of up to 7000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e4fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPUB_FILE_PATH = \"<INPUT FILE>.epub\"\n",
    "chapters = read_epub_by_chapters(EPUB_FILE_PATH)\n",
    "chapter_chunks = {}\n",
    "for title, text in chapters.items():\n",
    "    chapter_chunks[title] = split_text_to_chunks(text)\n",
    "print(f\"Chapters found: {list(chapter_chunks.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa166b7",
   "metadata": {},
   "source": [
    "## Generate audio\n",
    "We are now at the crux of the notebook where we iterate over each chunk of text and send it to Gemini 2.5 Flash Preview TTS to convert it to speech. After attaching a chapter name to each chunk, using the `save_audiobook()`, we save the speech chunks to a `m4b` file and then print a command that will attach the chapter metadata correctly to the `m4b` file using `ffmpeg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caae951",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pcm_chunks = []\n",
    "chapter_titles = []\n",
    "for chapter_title, chunks in chapter_chunks.items():\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        print(f\"Generating TTS for {chapter_title} - chunk {idx + 1}/{len(chunks)}\")\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.5-flash-preview-tts\",\n",
    "            contents=f\"{PROMPT_FOR_READING}: {chunk}\",\n",
    "            config=types.GenerateContentConfig(\n",
    "                response_modalities=[\"AUDIO\"],\n",
    "                speech_config=types.SpeechConfig(\n",
    "                    voice_config=types.VoiceConfig(\n",
    "                        prebuilt_voice_config=types.PrebuiltVoiceConfig(\n",
    "                            voice_name=\"Iapetus\",\n",
    "                        )\n",
    "                    )\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "        all_pcm_chunks.append(response.candidates[0].content.parts[0].inline_data.data)\n",
    "        chapter_titles.append(chapter_title)\n",
    "        time.sleep(120)  # Sleep to avoid hitting rate limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1226501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIOBOOK_FILE = \"<OUTPUT FILE>.m4b\"\n",
    "save_audiobook(AUDIOBOOK_FILE, all_pcm_chunks, chapter_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d67b69a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-explorations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
